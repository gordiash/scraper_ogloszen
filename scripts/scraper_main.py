#!/usr/bin/env python3
"""
GÅÃ“WNY SKRYPT SCRAPERA - KOMPLETNY PIPELINE
ÅÄ…czy scraping â†’ parsing adresÃ³w â†’ geocoding w jeden proces
"""
import logging
import sys
import os
import argparse
from datetime import datetime
from typing import List, Dict

# Dodaj gÅ‚Ã³wny katalog do Å›cieÅ¼ki
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.scrapers.otodom_scraper import get_otodom_listings
from src.parsers.address_parser import process_all_locations
from src.geocoding.geocoder import update_all_coordinates_improved
from supabase_utils import save_listings_to_supabase, get_supabase_client

# Konfiguracja logowania
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

def print_banner():
    """WyÅ›wietl banner aplikacji"""
    print("="*80)
    print("ğŸ  SCRAPER NIERUCHOMOÅšCI - KOMPLETNY PIPELINE")
    print("="*80)
    print("ğŸ“… Data uruchomienia:", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("ğŸ”— Å¹rÃ³dÅ‚o: Otodom.pl")
    print("ğŸ’¾ Baza danych: Supabase")
    print("ğŸŒ Geocoding: OpenStreetMap Nominatim")
    print("="*80)

def get_database_stats() -> Dict[str, int]:
    """Pobierz statystyki z bazy danych"""
    try:
        supabase = get_supabase_client()
        
        # Statystyki listings
        listings_result = supabase.table("listings").select("id", count="exact").execute()
        total_listings = listings_result.count
        
        # Statystyki addresses
        addresses_result = supabase.table("addresses").select("id", count="exact").execute()
        total_addresses = addresses_result.count
        
        # Statystyki geocoded
        geocoded_result = supabase.table("addresses").select("id", count="exact").not_.is_("latitude", "null").execute()
        geocoded_count = geocoded_result.count
        
        return {
            "total_listings": total_listings,
            "total_addresses": total_addresses,
            "geocoded_count": geocoded_count
        }
        
    except Exception as e:
        logger.error(f"âŒ BÅ‚Ä…d pobierania statystyk: {e}")
        return {
            "total_listings": 0,
            "total_addresses": 0,
            "geocoded_count": 0
        }

def print_stats(title: str, stats: Dict[str, int]):
    """WyÅ›wietl statystyki"""
    print(f"\nğŸ“Š {title}")
    print("-" * 60)
    print(f"ğŸ“‹ ÅÄ…cznie ogÅ‚oszeÅ„: {stats['total_listings']:,}")
    print(f"ğŸ“ ÅÄ…cznie adresÃ³w: {stats['total_addresses']:,}")
    print(f"ğŸŒ Z wspÃ³Å‚rzÄ™dnymi: {stats['geocoded_count']:,}")
    
    if stats['total_addresses'] > 0:
        geocoding_rate = (stats['geocoded_count'] / stats['total_addresses']) * 100
        print(f"ğŸ“ˆ Pokrycie geocoding: {geocoding_rate:.1f}%")

def run_scraping_phase(max_pages: int) -> List[Dict]:
    """
    Faza 1: Scrapowanie ogÅ‚oszeÅ„ z Otodom.pl
    
    Args:
        max_pages: Maksymalna liczba stron do scrapowania
    
    Returns:
        List[Dict]: Lista pobranych ogÅ‚oszeÅ„
    """
    print(f"\nğŸ” FAZA 1: SCRAPOWANIE OTODOM.PL")
    print(f"ğŸ“„ Maksymalna liczba stron: {max_pages}")
    print("-" * 60)
    
    try:
        listings = get_otodom_listings(max_pages=max_pages)
        
        if listings:
            print(f"âœ… Pobrano {len(listings)} ogÅ‚oszeÅ„ z Otodom.pl")
            
            # Statystyki jakoÅ›ci danych
            with_price = len([l for l in listings if l.get('price')])
            with_location = len([l for l in listings if l.get('location')])
            with_area = len([l for l in listings if l.get('area')])
            
            print(f"ğŸ’° Z cenami: {with_price}/{len(listings)} ({with_price/len(listings)*100:.1f}%)")
            print(f"ğŸ“ Z lokalizacjÄ…: {with_location}/{len(listings)} ({with_location/len(listings)*100:.1f}%)")
            print(f"ğŸ“ Z powierzchniÄ…: {with_area}/{len(listings)} ({with_area/len(listings)*100:.1f}%)")
            
            return listings
        else:
            print("âŒ Nie pobrano Å¼adnych ogÅ‚oszeÅ„")
            return []
            
    except Exception as e:
        logger.error(f"âŒ BÅ‚Ä…d w fazie scrapowania: {e}")
        return []

def run_saving_phase(listings: List[Dict]) -> int:
    """
    Faza 2: Zapis ogÅ‚oszeÅ„ do bazy danych
    
    Args:
        listings: Lista ogÅ‚oszeÅ„ do zapisu
    
    Returns:
        int: Liczba zapisanych ogÅ‚oszeÅ„
    """
    print(f"\nğŸ’¾ FAZA 2: ZAPIS DO BAZY DANYCH")
    print(f"ğŸ“‹ OgÅ‚oszeÅ„ do zapisu: {len(listings)}")
    print("-" * 60)
    
    try:
        saved_count = save_listings_to_supabase(listings)
        
        if saved_count > 0:
            print(f"âœ… Zapisano {saved_count} nowych ogÅ‚oszeÅ„")
            
            # Statystyki zapisu
            duplicate_count = len(listings) - saved_count
            if duplicate_count > 0:
                print(f"â­ï¸ PominiÄ™to {duplicate_count} duplikatÃ³w")
                
            success_rate = (saved_count / len(listings)) * 100
            print(f"ğŸ“ˆ SkutecznoÅ›Ä‡ zapisu: {success_rate:.1f}%")
        else:
            print("âš ï¸ Nie zapisano Å¼adnych nowych ogÅ‚oszeÅ„ (wszystkie to duplikaty)")
            
        return saved_count
        
    except Exception as e:
        logger.error(f"âŒ BÅ‚Ä…d w fazie zapisu: {e}")
        return 0

def run_parsing_phase() -> bool:
    """
    Faza 3: Parsing adresÃ³w
    
    Returns:
        bool: True jeÅ›li parsing siÄ™ udaÅ‚
    """
    print(f"\nğŸ“ FAZA 3: PARSING ADRESÃ“W")
    print("-" * 60)
    
    try:
        # Uruchom parsing adresÃ³w (funkcja wyÅ›wietla wÅ‚asne statystyki)
        process_all_locations()
        print("âœ… Parsing adresÃ³w zakoÅ„czony")
        return True
        
    except Exception as e:
        logger.error(f"âŒ BÅ‚Ä…d w fazie parsingu adresÃ³w: {e}")
        return False

def run_geocoding_phase(max_addresses: int) -> bool:
    """
    Faza 4: Geocoding - uzupeÅ‚nianie wspÃ³Å‚rzÄ™dnych
    
    Args:
        max_addresses: Maksymalna liczba adresÃ³w do geocodingu
    
    Returns:
        bool: True jeÅ›li geocoding siÄ™ udaÅ‚
    """
    print(f"\nğŸŒ FAZA 4: GEOCODING WSPÃ“ÅRZÄ˜DNYCH")
    print(f"ğŸ“ Maksymalna liczba adresÃ³w: {max_addresses}")
    print("-" * 60)
    
    try:
        # Uruchom geocoding (funkcja wyÅ›wietla wÅ‚asne statystyki)
        update_all_coordinates_improved(
            batch_size=50,
            max_addresses=max_addresses
        )
        print("âœ… Geocoding zakoÅ„czony")
        return True
        
    except Exception as e:
        logger.error(f"âŒ BÅ‚Ä…d w fazie geocodingu: {e}")
        return False

def run_complete_pipeline(max_pages: int = 5, max_geocoding_addresses: int = 100) -> bool:
    """
    Uruchom kompletny pipeline scrapera
    
    Args:
        max_pages: Maksymalna liczba stron do scrapowania
        max_geocoding_addresses: Maksymalna liczba adresÃ³w do geocodingu
    
    Returns:
        bool: True jeÅ›li caÅ‚y pipeline siÄ™ udaÅ‚
    """
    print_banner()
    
    # Statystyki poczÄ…tkowe
    initial_stats = get_database_stats()
    print_stats("STATYSTYKI POCZÄ„TKOWE", initial_stats)
    
    success_phases = 0
    total_phases = 4
    
    # FAZA 1: Scrapowanie
    listings = run_scraping_phase(max_pages)
    if listings:
        success_phases += 1
    
    # FAZA 2: Zapis do bazy
    if listings:
        saved_count = run_saving_phase(listings)
        if saved_count >= 0:  # 0 teÅ¼ jest sukcesem (duplikaty)
            success_phases += 1
    else:
        print("\nğŸ’¾ FAZA 2: POMINIÄ˜TO - brak ogÅ‚oszeÅ„ do zapisu")
    
    # FAZA 3: Parsing adresÃ³w
    if run_parsing_phase():
        success_phases += 1
    
    # FAZA 4: Geocoding
    if run_geocoding_phase(max_geocoding_addresses):
        success_phases += 1
    
    # Statystyki koÅ„cowe
    final_stats = get_database_stats()
    print_stats("STATYSTYKI KOÅƒCOWE", final_stats)
    
    # Podsumowanie zmian
    print(f"\nğŸ“ˆ ZMIANY W BAZIE DANYCH")
    print("-" * 60)
    print(f"ğŸ“‹ Nowe ogÅ‚oszenia: +{final_stats['total_listings'] - initial_stats['total_listings']}")
    print(f"ğŸ“ Nowe adresy: +{final_stats['total_addresses'] - initial_stats['total_addresses']}")
    print(f"ğŸŒ Nowe wspÃ³Å‚rzÄ™dne: +{final_stats['geocoded_count'] - initial_stats['geocoded_count']}")
    
    # Podsumowanie pipeline
    print(f"\nğŸ¯ PODSUMOWANIE PIPELINE")
    print("=" * 80)
    print(f"âœ… Udane fazy: {success_phases}/{total_phases}")
    print(f"ğŸ“ˆ SkutecznoÅ›Ä‡: {(success_phases/total_phases)*100:.1f}%")
    
    if success_phases == total_phases:
        print("ğŸ‰ PIPELINE ZAKOÅƒCZONY SUKCESEM!")
        return True
    else:
        print("âš ï¸ Pipeline zakoÅ„czony z bÅ‚Ä™dami")
        return False

def main():
    """GÅ‚Ã³wna funkcja z argumentami CLI"""
    parser = argparse.ArgumentParser(
        description='Kompletny pipeline scrapera nieruchomoÅ›ci',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
PrzykÅ‚ady uÅ¼ycia:
  python scripts/scraper_main.py                           # DomyÅ›lne ustawienia
  python scripts/scraper_main.py --pages 10               # 10 stron Otodom
  python scripts/scraper_main.py --geocoding-limit 200    # Max 200 adresÃ³w do geocodingu
  python scripts/scraper_main.py --pages 3 --geocoding-limit 50  # Kombinacja
        """
    )
    
    parser.add_argument(
        '--pages',
        type=int,
        default=5,
        help='Maksymalna liczba stron do scrapowania z Otodom.pl (domyÅ›lnie: 5)'
    )
    
    parser.add_argument(
        '--geocoding-limit',
        type=int,
        default=100,
        help='Maksymalna liczba adresÃ³w do geocodingu (domyÅ›lnie: 100)'
    )
    
    parser.add_argument(
        '--skip-scraping',
        action='store_true',
        help='PomiÅ„ scrapowanie, uruchom tylko parsing i geocoding'
    )
    
    parser.add_argument(
        '--skip-geocoding',
        action='store_true',
        help='PomiÅ„ geocoding, uruchom tylko scrapowanie i parsing'
    )
    
    parser.add_argument(
        '--stats-only',
        action='store_true',
        help='PokaÅ¼ tylko statystyki bazy danych'
    )
    
    args = parser.parse_args()
    
    try:
        if args.stats_only:
            # Tylko statystyki
            print_banner()
            stats = get_database_stats()
            print_stats("AKTUALNE STATYSTYKI", stats)
            return
        
        # Walidacja argumentÃ³w
        if args.pages < 1 or args.pages > 20:
            print("âŒ Liczba stron musi byÄ‡ miÄ™dzy 1 a 20")
            sys.exit(1)
            
        if args.geocoding_limit < 1 or args.geocoding_limit > 1000:
            print("âŒ Limit geocodingu musi byÄ‡ miÄ™dzy 1 a 1000")
            sys.exit(1)
        
        # Uruchom pipeline
        if args.skip_scraping and args.skip_geocoding:
            print("âŒ Nie moÅ¼na pominÄ…Ä‡ zarÃ³wno scrapowania jak i geocodingu")
            sys.exit(1)
        
        # Modyfikowany pipeline
        if args.skip_scraping:
            print_banner()
            print("â­ï¸ Pomijam scrapowanie - uruchamiam tylko parsing i geocoding")
            
            success = True
            if run_parsing_phase():
                if not args.skip_geocoding:
                    success = run_geocoding_phase(args.geocoding_limit)
            else:
                success = False
                
        elif args.skip_geocoding:
            print_banner()
            print("â­ï¸ Pomijam geocoding - uruchamiam tylko scrapowanie i parsing")
            
            listings = run_scraping_phase(args.pages)
            success = False
            
            if listings:
                saved_count = run_saving_phase(listings)
                if saved_count >= 0:
                    success = run_parsing_phase()
        else:
            # PeÅ‚ny pipeline
            success = run_complete_pipeline(
                max_pages=args.pages,
                max_geocoding_addresses=args.geocoding_limit
            )
        
        # Kod wyjÅ›cia
        sys.exit(0 if success else 1)
        
    except KeyboardInterrupt:
        print("\nâš ï¸ Pipeline przerwany przez uÅ¼ytkownika")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ BÅ‚Ä…d krytyczny w pipeline: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 