"""
GÅ‚Ã³wny scraper nieruchomoÅ›ci - pobiera dane ze wszystkich portali z deduplikacjÄ…
"""
import logging
import sys
from datetime import datetime
from typing import List, Dict

# Import scraperÃ³w
from otodom_only_scraper import get_otodom_listings
# from scrapers.freedom import get_freedom_listings  # USUNIÄ˜TY
# from scrapers.metrohouse import get_metrohouse_listings  # USUNIÄ˜TY  
# from scrapers.domiporta import get_domiporta_listings  # USUNIÄ˜TY
# from scrapers.gratka import get_gratka_listings  # USUNIÄ˜TY
# from scrapers.olx import get_olx_listings  # USUNIÄ˜TY

# Import utils
from utils import deduplicate_listings, generate_duplicate_report, find_duplicates

# Konfiguracja logowania
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper.log'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

# Konfiguracja scraperÃ³w - TYLKO DOSTÄ˜PNE
SCRAPERS = {
    "otodom": get_otodom_listings,
    # Inne scrapery usuniÄ™te - dodaj gdy bÄ™dÄ… gotowe
}

def run_scraper(scraper_name: str, max_pages: int = 2) -> List[Dict]:
    """
    Uruchamia scraper dla konkretnego portalu
    
    Args:
        scraper_name: Nazwa scrapera
        max_pages: Maksymalna liczba stron
    
    Returns:
        List[Dict]: Lista ogÅ‚oszeÅ„
    """
    if scraper_name not in SCRAPERS:
        logger.error(f"Nieznany scraper: {scraper_name}")
        return []
    
    try:
        logger.info(f"Rozpoczynam scraping: {scraper_name}")
        listings = SCRAPERS[scraper_name](max_pages)
        logger.info(f"ZakoÅ„czono scraping {scraper_name}: {len(listings)} ogÅ‚oszeÅ„")
        return listings
    except Exception as e:
        logger.error(f"BÅ‚Ä…d scrapera {scraper_name}: {e}")
        return []

def run_all_scrapers(max_pages: int = 2) -> Dict[str, List[Dict]]:
    """
    Uruchamia wszystkie scrapery
    
    Args:
        max_pages: Maksymalna liczba stron dla kaÅ¼dego portalu
    
    Returns:
        Dict: Wyniki z wszystkich scraperÃ³w
    """
    results = {}
    total_listings = 0
    
    for scraper_name in SCRAPERS.keys():
        try:
            listings = run_scraper(scraper_name, max_pages)
            results[scraper_name] = listings
            total_listings += len(listings)
        except Exception as e:
            logger.error(f"BÅ‚Ä…d uruchamiania scrapera {scraper_name}: {e}")
            results[scraper_name] = []
    
    logger.info(f"CaÅ‚kowita liczba pobranych ogÅ‚oszeÅ„: {total_listings}")
    return results

def deduplicate_all_listings(results: Dict[str, List[Dict]], 
                           similarity_threshold: float = 75.0) -> List[Dict]:
    """
    Usuwa duplikaty ze wszystkich pobranych ogÅ‚oszeÅ„
    
    Args:
        results: Wyniki scraperÃ³w
        similarity_threshold: PrÃ³g podobieÅ„stwa (0-100)
    
    Returns:
        List[Dict]: Lista unikatowych ogÅ‚oszeÅ„
    """
    logger.info("ğŸ” Rozpoczynam deduplikacjÄ™ ogÅ‚oszeÅ„...")
    
    # PoÅ‚Ä…cz wszystkie ogÅ‚oszenia w jednÄ… listÄ™
    all_listings = []
    for scraper_name, listings in results.items():
        for listing in listings:
            # Dodaj timestamp i metadata
            listing["scraped_at"] = datetime.now().isoformat()
            listing["scraper_version"] = "1.0"
        all_listings.extend(listings)
    
    if not all_listings:
        logger.warning("Brak ogÅ‚oszeÅ„ do deduplikacji")
        return []
    
    logger.info(f"ğŸ“Š ÅÄ…cznie ogÅ‚oszeÅ„ przed deduplikacjÄ…: {len(all_listings)}")
    
    # ZnajdÅº duplikaty
    unique_listings, duplicates = find_duplicates(all_listings, similarity_threshold)
    
    if duplicates:
        logger.info(f"ğŸ”„ Znaleziono {len(duplicates)} duplikatÃ³w:")
        
        # Statystyki duplikatÃ³w per portal
        duplicate_stats = {}
        for dup in duplicates:
            source = dup.get('source', 'nieznany')
            duplicate_stats[source] = duplicate_stats.get(source, 0) + 1
        
        for source, count in sorted(duplicate_stats.items()):
            logger.info(f"   â€¢ {source}: {count} duplikatÃ³w")
        
        # PokaÅ¼ przykÅ‚ady wysokopodobnych duplikatÃ³w
        high_similarity_dups = [d for d in duplicates if d.get('similarity_score', 0) >= 90]
        if high_similarity_dups:
            logger.info(f"ğŸ”´ Bardzo podobne ogÅ‚oszenia (90%+): {len(high_similarity_dups)}")
            for dup in high_similarity_dups[:3]:
                title = dup.get('title', 'Brak tytuÅ‚u')[:50]
                source = dup.get('source', '')
                similarity = dup.get('similarity_score', 0)
                logger.info(f"   â€¢ {title}... ({source}) - {similarity:.1f}%")
    
    # Deduplikacja z zachowaniem najlepszych ÅºrÃ³deÅ‚
    deduplicated = deduplicate_listings(all_listings, 
                                      similarity_threshold=similarity_threshold, 
                                      keep_best_source=True)
    
    logger.info(f"âœ… Po deduplikacji: {len(deduplicated)} unikatowych ogÅ‚oszeÅ„")
    logger.info(f"ğŸ§¹ UsuniÄ™to: {len(all_listings) - len(deduplicated)} duplikatÃ³w")
    
    return deduplicated

def save_all_to_supabase(unique_listings: List[Dict], require_complete: bool = True) -> None:
    """
    Zapisuje unikatowe ogÅ‚oszenia do Supabase
    
    Args:
        unique_listings: Lista unikatowych ogÅ‚oszeÅ„
        require_complete: Czy wymagaÄ‡ kompletnych danych
    """
    if not unique_listings:
        logger.warning("Brak ogÅ‚oszeÅ„ do zapisu")
        return
    
    logger.info(f"ğŸ’¾ ZapisujÄ™ {len(unique_listings)} unikatowych ogÅ‚oszeÅ„ do Supabase...")
    
    try:
        from supabase_utils import save_listings_to_supabase
        saved_count = save_listings_to_supabase(unique_listings, require_complete=require_complete)
        logger.info(f"âœ… Zapisano {saved_count}/{len(unique_listings)} ogÅ‚oszeÅ„")
        
        if saved_count < len(unique_listings):
            logger.warning(f"âš ï¸ Nie zapisano {len(unique_listings) - saved_count} ogÅ‚oszeÅ„ (prawdopodobnie juÅ¼ istniejÄ… lub walidacja odrzuciÅ‚a)")
        
    except Exception as e:
        logger.error(f"âŒ BÅ‚Ä…d zapisu do Supabase: {e}")

def print_summary(results: Dict[str, List[Dict]], unique_listings: List[Dict]) -> None:
    """WyÅ›wietla podsumowanie scraperÃ³w z informacjÄ… o deduplikacji"""
    print("\n" + "="*60)
    print("PODSUMOWANIE SCRAPERÃ“W NIERUCHOMOÅšCI Z DEDUPLIKACJÄ„")
    print("="*60)
    
    total_raw = 0
    print("ğŸ“Š OgÅ‚oszenia per portal (przed deduplikacjÄ…):")
    for scraper_name, listings in results.items():
        count = len(listings)
        total_raw += count
        status = "âœ“" if count > 0 else "âœ—"
        print(f"  {status} {scraper_name:12} : {count:3} ogÅ‚oszeÅ„")
    
    print("-"*60)
    print(f"ğŸ“‹ RAZEM przed deduplikacjÄ…: {total_raw} ogÅ‚oszeÅ„")
    print(f"ğŸ§¹ RAZEM po deduplikacji:  {len(unique_listings)} unikatowych ogÅ‚oszeÅ„")
    print(f"ğŸ”„ UsuniÄ™to duplikatÃ³w:    {total_raw - len(unique_listings)}")
    
    # Statystyki koÅ„cowe per portal
    if unique_listings:
        print("\nğŸ“ˆ RozkÅ‚ad unikatowych ogÅ‚oszeÅ„ per portal:")
        unique_stats = {}
        for listing in unique_listings:
            source = listing.get('source', 'nieznany')
            unique_stats[source] = unique_stats.get(source, 0) + 1
        
        for source, count in sorted(unique_stats.items()):
            percentage = (count / len(unique_listings)) * 100
            print(f"  â€¢ {source:12} : {count:3} ogÅ‚oszeÅ„ ({percentage:.1f}%)")
    
    print("="*60)

def main():
    """GÅ‚Ã³wna funkcja programu z deduplikacjÄ… i argumentami wiersza poleceÅ„"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Scraper nieruchomoÅ›ci - wszystkie portale')
    parser.add_argument('--pages', type=int, default=2, help='Maksymalna liczba stron per portal (domyÅ›lnie: 2)')
    parser.add_argument('--save_db', action='store_true', help='Zapisz do bazy danych Supabase')
    parser.add_argument('--no-validation', action='store_true', help='WyÅ‚Ä…cz walidacjÄ™ kompletnoÅ›ci danych')
    parser.add_argument('--quiet', action='store_true', help='Tryb cichy (mniej logÃ³w)')
    
    args = parser.parse_args()
    
    # Ustaw poziom logowania
    if args.quiet:
        logging.getLogger().setLevel(logging.WARNING)
    
    logger.info("ğŸš€ Rozpoczynam scraping portali nieruchomoÅ›ci z deduplikacjÄ…")
    logger.info(f"ğŸ“Š Parametry: pages={args.pages}, save_db={args.save_db}, validation={'off' if args.no_validation else 'on'}")
    
    # Uruchom wszystkie scrapery
    results = run_all_scrapers(max_pages=args.pages)
    
    # Deduplikacja ogÅ‚oszeÅ„ miÄ™dzy portalami
    unique_listings = deduplicate_all_listings(results, similarity_threshold=75.0)
    
    # WyÅ›wietl podsumowanie
    print_summary(results, unique_listings)
    
    # Zapisz unikatowe ogÅ‚oszenia do Supabase (opcjonalnie)
    if args.save_db and unique_listings:
        try:
            save_all_to_supabase(unique_listings, require_complete=not args.no_validation)
        except Exception as e:
            logger.error(f"âŒ BÅ‚Ä…d zapisu do Supabase: {e}")
    elif args.save_db:
        logger.warning("âš ï¸ Brak unikatowych ogÅ‚oszeÅ„ do zapisu")
    else:
        logger.info("â„¹ï¸ PominiÄ™to zapis do bazy (uÅ¼yj --save_db aby zapisaÄ‡)")
    
    logger.info("âœ… ZakoÅ„czono scraping z deduplikacjÄ…")

if __name__ == "__main__":
    main() 