#!/usr/bin/env python3
"""
TEST GÅÃ“WNEGO SCRAPERA Z DEDUPLIKACJÄ„ (BEZ SUPABASE)
Symuluje dziaÅ‚anie main.py ale bez zapisu do bazy danych
"""
import logging
import sys
from datetime import datetime
from typing import List, Dict

# Import scraperÃ³w
from scrapers.freedom import get_freedom_listings
from scrapers.otodom import get_otodom_listings
from scrapers.metrohouse import get_metrohouse_listings
from scrapers.domiporta import get_domiporta_listings
from scrapers.gratka import get_gratka_listings
from scrapers.olx import get_olx_listings

# Import utils
from utils import deduplicate_listings, generate_duplicate_report, find_duplicates

# Konfiguracja logowania
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

# Konfiguracja scraperÃ³w
SCRAPERS = {
    "freedom": get_freedom_listings,
    "otodom": get_otodom_listings,
    "metrohouse": get_metrohouse_listings,
    "domiporta": get_domiporta_listings,
    "gratka": get_gratka_listings,
    "olx": get_olx_listings,
}

def run_scraper(scraper_name: str, max_pages: int = 1) -> List[Dict]:
    """
    Uruchamia scraper dla konkretnego portalu
    
    Args:
        scraper_name: Nazwa scrapera
        max_pages: Maksymalna liczba stron
    
    Returns:
        List[Dict]: Lista ogÅ‚oszeÅ„
    """
    if scraper_name not in SCRAPERS:
        logger.error(f"Nieznany scraper: {scraper_name}")
        return []
    
    try:
        logger.info(f"ğŸš€ Rozpoczynam scraping: {scraper_name}")
        listings = SCRAPERS[scraper_name](max_pages)
        logger.info(f"âœ… ZakoÅ„czono scraping {scraper_name}: {len(listings)} ogÅ‚oszeÅ„")
        return listings
    except Exception as e:
        logger.error(f"âŒ BÅ‚Ä…d scrapera {scraper_name}: {e}")
        return []

def run_all_scrapers(max_pages: int = 1) -> Dict[str, List[Dict]]:
    """
    Uruchamia wszystkie scrapery
    
    Args:
        max_pages: Maksymalna liczba stron dla kaÅ¼dego portalu
    
    Returns:
        Dict: Wyniki z wszystkich scraperÃ³w
    """
    results = {}
    total_listings = 0
    
    print("="*80)
    print("ğŸ  GÅÃ“WNY SCRAPER Z DEDUPLIKACJÄ„ - TEST")
    print("="*80)
    
    for scraper_name in SCRAPERS.keys():
        try:
            listings = run_scraper(scraper_name, max_pages)
            results[scraper_name] = listings
            total_listings += len(listings)
        except Exception as e:
            logger.error(f"âŒ BÅ‚Ä…d uruchamiania scrapera {scraper_name}: {e}")
            results[scraper_name] = []
    
    print(f"\nğŸ“Š Podsumowanie pobierania:")
    print(f"   ğŸ  CaÅ‚kowita liczba pobranych ogÅ‚oszeÅ„: {total_listings}")
    return results

def deduplicate_all_listings(results: Dict[str, List[Dict]], 
                           similarity_threshold: float = 75.0) -> List[Dict]:
    """
    Usuwa duplikaty ze wszystkich pobranych ogÅ‚oszeÅ„
    
    Args:
        results: Wyniki scraperÃ³w
        similarity_threshold: PrÃ³g podobieÅ„stwa (0-100)
    
    Returns:
        List[Dict]: Lista unikatowych ogÅ‚oszeÅ„
    """
    print(f"\n{'='*60}")
    print("ğŸ” DEDUPLIKACJA OGÅOSZEÅƒ")
    print("="*60)
    
    logger.info("ğŸ” Rozpoczynam deduplikacjÄ™ ogÅ‚oszeÅ„...")
    
    # PoÅ‚Ä…cz wszystkie ogÅ‚oszenia w jednÄ… listÄ™
    all_listings = []
    for scraper_name, listings in results.items():
        for listing in listings:
            # Dodaj timestamp i metadata
            listing["scraped_at"] = datetime.now().isoformat()
            listing["scraper_version"] = "1.0"
        all_listings.extend(listings)
    
    if not all_listings:
        logger.warning("âŒ Brak ogÅ‚oszeÅ„ do deduplikacji")
        return []
    
    logger.info(f"ğŸ“Š ÅÄ…cznie ogÅ‚oszeÅ„ przed deduplikacjÄ…: {len(all_listings)}")
    
    # ZnajdÅº duplikaty
    unique_listings, duplicates = find_duplicates(all_listings, similarity_threshold)
    
    if duplicates:
        logger.info(f"ğŸ”„ Znaleziono {len(duplicates)} duplikatÃ³w:")
        
        # Statystyki duplikatÃ³w per portal
        duplicate_stats = {}
        for dup in duplicates:
            source = dup.get('source', 'nieznany')
            duplicate_stats[source] = duplicate_stats.get(source, 0) + 1
        
        for source, count in sorted(duplicate_stats.items()):
            logger.info(f"   â€¢ {source}: {count} duplikatÃ³w")
        
        # PokaÅ¼ przykÅ‚ady wysokopodobnych duplikatÃ³w
        high_similarity_dups = [d for d in duplicates if d.get('similarity_score', 0) >= 90]
        if high_similarity_dups:
            logger.info(f"ğŸ”´ Bardzo podobne ogÅ‚oszenia (90%+): {len(high_similarity_dups)}")
            for dup in high_similarity_dups[:3]:
                title = dup.get('title', 'Brak tytuÅ‚u')[:50]
                source = dup.get('source', '')
                similarity = dup.get('similarity_score', 0)
                logger.info(f"   â€¢ {title}... ({source}) - {similarity:.1f}%")
    
    # Deduplikacja z zachowaniem najlepszych ÅºrÃ³deÅ‚
    deduplicated = deduplicate_listings(all_listings, 
                                      similarity_threshold=similarity_threshold, 
                                      keep_best_source=True)
    
    logger.info(f"âœ… Po deduplikacji: {len(deduplicated)} unikatowych ogÅ‚oszeÅ„")
    logger.info(f"ğŸ§¹ UsuniÄ™to: {len(all_listings) - len(deduplicated)} duplikatÃ³w")
    
    return deduplicated

def print_final_summary(results: Dict[str, List[Dict]], unique_listings: List[Dict]) -> None:
    """WyÅ›wietla koÅ„cowe podsumowanie"""
    print(f"\n{'='*80}")
    print("ğŸ“Š KOÅƒCOWE PODSUMOWANIE SCRAPERA Z DEDUPLIKACJÄ„")
    print("="*80)
    
    total_raw = 0
    print("ğŸ“‹ OgÅ‚oszenia per portal (przed deduplikacjÄ…):")
    for scraper_name, listings in results.items():
        count = len(listings)
        total_raw += count
        status = "âœ“" if count > 0 else "âœ—"
        print(f"  {status} {scraper_name:12} : {count:3} ogÅ‚oszeÅ„")
    
    print("-"*80)
    print(f"ğŸ“Š RAZEM przed deduplikacjÄ…: {total_raw} ogÅ‚oszeÅ„")
    print(f"ğŸ§¹ RAZEM po deduplikacji:  {len(unique_listings)} unikatowych ogÅ‚oszeÅ„")
    
    if total_raw > 0:
        reduction_percentage = ((total_raw - len(unique_listings)) / total_raw) * 100
        print(f"ğŸ”„ SkutecznoÅ›Ä‡ deduplikacji: {reduction_percentage:.1f}% duplikatÃ³w usuniÄ™to")
    
    # Statystyki koÅ„cowe per portal
    if unique_listings:
        print("\nğŸ“ˆ RozkÅ‚ad unikatowych ogÅ‚oszeÅ„ per portal:")
        unique_stats = {}
        listings_with_price = 0
        total_price = 0
        
        for listing in unique_listings:
            source = listing.get('source', 'nieznany')
            unique_stats[source] = unique_stats.get(source, 0) + 1
            
            # Statystyki cen
            if listing.get('price'):
                listings_with_price += 1
                total_price += listing['price']
        
        for source, count in sorted(unique_stats.items()):
            percentage = (count / len(unique_listings)) * 100
            print(f"  â€¢ {source:12} : {count:3} ogÅ‚oszeÅ„ ({percentage:.1f}%)")
        
        print(f"\nğŸ’° Statystyki cenowe:")
        print(f"  â€¢ OgÅ‚oszenia z cenami: {listings_with_price}/{len(unique_listings)}")
        if listings_with_price > 0:
            avg_price = total_price / listings_with_price
            print(f"  â€¢ Åšrednia cena: {avg_price:,.0f} zÅ‚")
    
    # NajdroÅ¼sze oferty
    expensive_listings = [l for l in unique_listings if l.get('price') and l['price'] > 500000]
    if expensive_listings:
        expensive_listings.sort(key=lambda x: x['price'], reverse=True)
        print(f"\nğŸ’ TOP 3 najdroÅ¼sze unikatowe oferty:")
        for i, listing in enumerate(expensive_listings[:3]):
            price = listing['price']
            currency = listing.get('price_currency', 'zÅ‚')
            title = listing.get('title', 'Brak tytuÅ‚u')[:50]
            source = listing.get('source', '')
            print(f"  {i+1}. {price:,.0f} {currency} - {title}... ({source})")
    
    print("="*80)

def main():
    """GÅ‚Ã³wna funkcja testowa"""
    try:
        # Uruchom wszystkie scrapery (tylko 1 strona dla szybkoÅ›ci)
        results = run_all_scrapers(max_pages=1)
        
        # Deduplikacja ogÅ‚oszeÅ„ miÄ™dzy portalami
        unique_listings = deduplicate_all_listings(results, similarity_threshold=75.0)
        
        # WyÅ›wietl koÅ„cowe podsumowanie
        print_final_summary(results, unique_listings)
        
        # Raport o duplikatach
        if unique_listings:
            _, duplicates = find_duplicates(
                [listing for listings in results.values() for listing in listings], 
                75.0
            )
            
            if duplicates:
                print(f"\n{'='*60}")
                print("ğŸ“„ RAPORT DUPLIKATÃ“W")
                print("="*60)
                report = generate_duplicate_report(duplicates)
                print(report)
        
        print(f"\nğŸ‰ SUKCES! Test gÅ‚Ã³wnego scrapera z deduplikacjÄ… zakoÅ„czony!")
        print(f"âœ… Pobrano {len(unique_listings)} unikatowych ogÅ‚oszeÅ„")
        print("\nğŸ’¡ Aby uruchomiÄ‡ z zapisem do Supabase:")
        print("   1. Skonfiguruj zmienne Å›rodowiskowe Supabase")
        print("   2. Uruchom: python main.py")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ Test przerwany przez uÅ¼ytkownika")
    except Exception as e:
        print(f"\nâŒ BÅ‚Ä…d w teÅ›cie: {e}")
        logging.error(f"BÅ‚Ä…d w test_main_with_deduplication: {e}", exc_info=True)

if __name__ == "__main__":
    main() 